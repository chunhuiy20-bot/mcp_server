# ============ LLM æ‰§è¡Œå™¨ ============
from typing import Any
from openai import AsyncOpenAI
from pydantic import BaseModel
from agent.workflow.node.node_config.LLMConfig import LLMConfig
from agent.workflow.node.node_executor_strategy.NodeExecutor import NodeExecutor
from utils.common.factory.DynamicModelFactory import dynamic_model_factory


# noinspection PyMethodMayBeStatic
class LLMExecutor(NodeExecutor):
    """LLM æ‰§è¡Œå™¨"""

    def __init__(self):
        self.client: AsyncOpenAI | None = None

    def _init_llm_client(self, config: LLMConfig):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if self.client is None:
            self.client = AsyncOpenAI(api_key=config.openai_api_key, base_url=config.openai_api_base)
        return self.client

    def _get_output_schema(self, config: LLMConfig) -> BaseModel:
        """è·å–è¾“å‡ºç»“æ„"""
        output_schema = dynamic_model_factory.create(config=config.output_schema, model_name="CustomOutputModel")
        # ========== è°ƒè¯•ä»£ç ï¼šæ£€æŸ¥æ¨¡å‹ç»“æ„ ==========
        print("\n" + "=" * 50)
        print("ğŸ“‹ ç”Ÿæˆçš„ Pydantic æ¨¡å‹ç»“æ„:")
        print("=" * 50)

        # 1. æ‰“å°æ¨¡å‹åç§°
        print(f"æ¨¡å‹åç§°: {output_schema.__name__}")

        # 2. æ‰“å°æ‰€æœ‰å­—æ®µåŠå…¶ç±»å‹
        print("\nå­—æ®µåˆ—è¡¨:")
        for field_name, field_info in output_schema.model_fields.items():
            print(f"  - {field_name}: {field_info.annotation}")
            if field_info.description:
                print(f"    æè¿°: {field_info.description}")

        # 3. æ‰“å°å®Œæ•´çš„ JSON Schemaï¼ˆæœ€è¯¦ç»†ï¼‰
        print("\nå®Œæ•´ JSON Schema:")
        import json
        schema = output_schema.model_json_schema()
        print(json.dumps(schema, indent=2, ensure_ascii=False))

        # 4. æ£€æŸ¥åµŒå¥—æ¨¡å‹çš„å­—æ®µç±»å‹
        print("\nåµŒå¥—å­—æ®µè¯¦ç»†æ£€æŸ¥:")
        if hasattr(output_schema, 'model_fields'):
            identity_field = output_schema.model_fields.get('identity')
            if identity_field:
                print(f"  identity ç±»å‹: {identity_field.annotation}")
                # æ£€æŸ¥ identity çš„å­å­—æ®µ
                if hasattr(identity_field.annotation, 'model_fields'):
                    print(f"  identity å­å­—æ®µ:")
                    for sub_name, sub_field in identity_field.annotation.model_fields.items():
                        print(f"    - {sub_name}: {sub_field.annotation}")

        print("=" * 50 + "\n")
        # ========== è°ƒè¯•ä»£ç ç»“æŸ ==========

        return output_schema

    def _handle_input_data(self, input_data: Any, config: LLMConfig) -> list:
        """å¤„ç†è¾“å…¥çš„æ¶ˆæ¯"""
        messages = [{"role": "system", "content": config.system_prompt}]

        print(f"\n[DEBUG] _handle_input_data æ¥æ”¶åˆ°çš„ input_data ç±»å‹: {type(input_data)}")
        print(f"[DEBUG] input_data å†…å®¹: {input_data}")

        # å¤„ç†ä¸åŒè¾“å…¥æ ¼å¼
        if isinstance(input_data, dict):
            # ä» state æå–æ¶ˆæ¯å†å²
            for msg in input_data.get("messages", []):
                if hasattr(msg, "content"):
                    role = "assistant" if "AI" in msg.__class__.__name__ else "user"
                    messages.append({"role": role, "content": msg.content})
                elif isinstance(msg, dict):
                    # ç¡®ä¿ role å’Œ content å­˜åœ¨ä¸”ä¸ä¸ºç©º
                    if msg.get("role") and msg.get("content") is not None:
                        messages.append(msg)
                    else:
                        print(f"[WARNING] è·³è¿‡æ— æ•ˆæ¶ˆæ¯: {msg}")
        elif isinstance(input_data, str):
            messages.append({"role": "user", "content": input_data})
        elif isinstance(input_data, list):
            # å¤„ç†åˆ—è¡¨æ ¼å¼çš„æ¶ˆæ¯
            for msg in input_data:
                if isinstance(msg, dict):
                    # ç¡®ä¿ role å’Œ content å­˜åœ¨ä¸”ä¸ä¸ºç©º
                    if msg.get("role") and msg.get("content") is not None:
                        messages.append(msg)
                    else:
                        print(f"[WARNING] è·³è¿‡æ— æ•ˆæ¶ˆæ¯: {msg}")
                elif hasattr(msg, "content"):
                    role = "assistant" if "AI" in msg.__class__.__name__ else "user"
                    messages.append({"role": role, "content": msg.content})

        print(f"[DEBUG] æœ€ç»ˆæ„é€ çš„ messages: {messages}\n")
        return messages

    async def execute(self, input_data: Any, config: LLMConfig) -> Any:
        """æ‰§è¡Œ LLM èŠ‚ç‚¹"""
        self._init_llm_client(config)
        messages = self._handle_input_data(input_data=input_data, config=config)
        if config.need_structure_output:
            response = await self.client.chat.completions.parse(
                model=config.model,
                messages=messages,
                temperature=config.temperature,
                response_format=self._get_output_schema(config)
            )
            print("æ ¼å¼åŒ–è¾“å‡º")
            print(response.choices[0].message.parsed)
            return response.choices[0].message.parsed
        else:
            response = await self.client.chat.completions.create(
                model=config.model,
                messages=messages,
                temperature=config.temperature
            )
            print("éæ ¼å¼åŒ–è¾“å‡º")
            print(response.choices[0].message.content)
            return response.choices[0].message.content
